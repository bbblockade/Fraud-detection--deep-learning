{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Masking\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "features_d1 = []\n",
    "labels_d1 = []\n",
    "# Open file for reading\n",
    "with open('domain1_train.json', 'r') as f:\n",
    "    for line in f:\n",
    "        # Parse the JSON line into a Python dictionary\n",
    "        obj = json.loads(line)\n",
    "        features_d1.append(obj['text'])\n",
    "        labels_d1.append(obj['label'])\n",
    "features_d2 = []\n",
    "labels_d2 = []\n",
    "# Open file for reading\n",
    "with open('domain2_train.json', 'r') as f:\n",
    "    for line in f:\n",
    "        # Parse the JSON line into a Python dictionary\n",
    "        obj = json.loads(line)\n",
    "        features_d2.append(obj['text'])\n",
    "        labels_d2.append(obj['label'])\n",
    "features = features_d1 + features_d2\n",
    "labels = labels_d1 + labels_d2\n",
    "new_features = []\n",
    "for text in features:\n",
    "    new_text = [x + 1 for x in text]\n",
    "    new_features.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "padded_features = pad_sequences(new_features, padding='pre', value=0)\n",
    "labels = to_categorical(labels, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1075"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, None, 128)         640000    \n",
      "                                                                 \n",
      " masking_4 (Masking)         (None, None, 128)         0         \n",
      "                                                                 \n",
      " conv1d_15 (Conv1D)          (None, None, 16)          6160      \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, None, 16)         64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_15 (MaxPoolin  (None, None, 16)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_16 (Conv1D)          (None, None, 16)          2320      \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, None, 16)         64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_16 (MaxPoolin  (None, None, 16)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, None, 16)          0         \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirecti  (None, 16)               1600      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 650,242\n",
      "Trainable params: 650,178\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Masking, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization, Bidirectional, concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "vocab_size = 5000  # Adjust based on your actual vocabulary size\n",
    "embedding_dim = 128\n",
    "\n",
    "\n",
    "n_classes = 2  # Binary classification\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 5000  # Adjust based on your actual vocabulary size\n",
    "embedding_dim = 128\n",
    "sequence_length = 250\n",
    "\n",
    "n_classes = 2  # Binary classification\n",
    "\n",
    "# Include early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, mask_zero=True),\n",
    "    Masking(mask_value=0),\n",
    "    Conv1D(16, 3, activation='relu', kernel_regularizer=l2(0.005)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(4),\n",
    "    Conv1D(16, 9, activation='relu', kernel_regularizer=l2(0.005)),  # Additional Conv1D layer\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(4),\n",
    "    Dropout(0.6),\n",
    "    Bidirectional(LSTM(8, recurrent_regularizer=l2(0.005), kernel_regularizer=l2(0.005))),\n",
    "    Dropout(0.6),\n",
    "    Dense(n_classes, activation='sigmoid', kernel_regularizer=l1(0.005))\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model to view the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 128)         640000    \n",
      "                                                                 \n",
      " masking_1 (Masking)         (None, None, 128)         0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 32)         18560     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 32)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 16)               2624      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 661,218\n",
      "Trainable params: 661,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Masking, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization, Bidirectional, concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Constants\n",
    "vocab_size = 5000  # Adjust based on your actual vocabulary size\n",
    "embedding_dim = 128\n",
    "sequence_length = 250\n",
    "n_classes = 2  # Assuming binary classification\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, mask_zero=True),\n",
    "    Masking(mask_value=0.0),\n",
    "    Bidirectional(LSTM(16, return_sequences=True, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01))),\n",
    "    Dropout(0.5),  # 50% dropout\n",
    "    Bidirectional(LSTM(8, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01))),\n",
    "    Dropout(0.5),  # 50% dropout\n",
    "    Dense(n_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "344/344 [==============================] - 271s 755ms/step - loss: 1.0655 - accuracy: 0.8000 - val_loss: 0.3947 - val_accuracy: 0.8897\n",
      "Epoch 2/50\n",
      "344/344 [==============================] - 246s 717ms/step - loss: 0.3850 - accuracy: 0.8952 - val_loss: 0.3482 - val_accuracy: 0.8912\n",
      "Epoch 3/50\n",
      "344/344 [==============================] - 245s 714ms/step - loss: 0.3392 - accuracy: 0.9065 - val_loss: 0.3494 - val_accuracy: 0.8841\n",
      "Epoch 4/50\n",
      "344/344 [==============================] - 246s 716ms/step - loss: 0.3111 - accuracy: 0.9137 - val_loss: 0.3476 - val_accuracy: 0.8843\n",
      "Epoch 5/50\n",
      "344/344 [==============================] - 483s 1s/step - loss: 0.2926 - accuracy: 0.9196 - val_loss: 0.3709 - val_accuracy: 0.8741\n",
      "Epoch 6/50\n",
      "344/344 [==============================] - 249s 726ms/step - loss: 0.2730 - accuracy: 0.9259 - val_loss: 0.3539 - val_accuracy: 0.8834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e027422be0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.2, epochs=50, callbacks=[early_stopping], batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 7s 99ms/step\n",
      "[0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1\n",
      " 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1\n",
      " 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1\n",
      " 1 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "test_ids = []\n",
    "test_texts = []\n",
    "# Open file for reading\n",
    "with open('test_set.json', 'r') as f:\n",
    "    for line in f:\n",
    "        # Parse the JSON line into a Python dictionary\n",
    "        obj = json.loads(line)\n",
    "        test_ids.append(obj['id'])\n",
    "        test_texts.append(obj['text'])\n",
    "test_features = pad_sequences(test_texts, padding='pre', value=0)\n",
    "import numpy as np\n",
    "# Make predictions\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "# Interpret predictions\n",
    "final_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df = pd.DataFrame({\"id\":test_ids, \"class\": final_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
